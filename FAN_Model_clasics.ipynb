{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGKeiy4Y9ZV9"
   },
   "source": [
    "## Conjunto de datos\n",
    "\n",
    "Para este avance se utilizara el dataset de \"Sentiment Analysis for Mental Health\" el cual esta compuesto por 53042 ejemplos con 7 clasificaciones diferentes\n",
    "\n",
    "https://www.kaggle.com/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iojl6mMO_ESA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rhuertap/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rhuertap/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rhuertap/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import util_reducido as util\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import psutil\n",
    "start_time = time.time()\n",
    "\n",
    "# obtencion de recursos\n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / 1024 / 1024  # En MB\n",
    "start_cpu = psutil.cpu_percent(interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8BnANYzZ_ESB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizaciones \n",
      "Modelos: \n",
      "finaliza el entrenamiento con lda\n",
      "finaliza el entrenamiento con vader\n",
      "finaliza el entrenamiento con tfidf\n",
      "finaliza el entrenamiento con lda\n",
      "finaliza el entrenamiento con vader\n",
      "finaliza el entrenamiento con tfidf\n",
      "finaliza el entrenamiento con lda  + knn\n",
      "finaliza el entrenamiento con vader  + knn\n",
      "finaliza el entrenamiento con tfidf + knn\n",
      "finaliza el entrenamiento con lda  + mlp\n",
      "finaliza el entrenamiento con vader  + mlp\n",
      "finaliza el entrenamiento con tfidf + mlp\n",
      "finaliza el entrenamiento con lda + xgb\n",
      "finaliza el entrenamiento con vader + xgb\n",
      "finaliza el entrenamiento con tfidf + xgb\n",
      "xgb - tfidf - f1-score: 0.658721711474121\n",
      "mlp - tfidf - f1-score: 0.6478589863435484\n",
      "logreg - tfidf - f1-score: 0.6358408098714543\n",
      "random_forest - lda - f1-score: 0.5158822839125463\n",
      "xgb - lda - f1-score: 0.49238380661777853\n",
      "knn - lda - f1-score: 0.4864926167254291\n",
      "random_forest - tfidf - f1-score: 0.46223579764665185\n",
      "mlp - lda - f1-score: 0.452254381081731\n",
      "knn - vader - f1-score: 0.3530046178344136\n",
      "random_forest - vader - f1-score: 0.29532844187251195\n",
      "logreg - lda - f1-score: 0.2618555255700512\n",
      "xgb - vader - f1-score: 0.24904615593905088\n",
      "mlp - vader - f1-score: 0.23077692049736184\n",
      "logreg - vader - f1-score: 0.15312059200817071\n",
      "knn - tfidf - f1-score: 0.12263887466556987\n",
      "\n",
      "Paso 1: analizando impacto de cada eliminación teniendo 13 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6863 (ΔF1 = -0.0177), buscando superar F1 = -1.0000\n",
      "   - Eliminando mlp_tfidf → F1 = 0.7018 (ΔF1 = -0.0022), buscando superar F1 = 0.6863\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7021 (ΔF1 = -0.0019), buscando superar F1 = 0.7018\n",
      "   - Eliminando random_forest_lda → F1 = 0.7026 (ΔF1 = -0.0013), buscando superar F1 = 0.7021\n",
      "   - Eliminando xgb_lda → F1 = 0.7025 (ΔF1 = -0.0015), buscando superar F1 = 0.7026\n",
      "   - Eliminando knn_lda → F1 = 0.7035 (ΔF1 = -0.0005), buscando superar F1 = 0.7026\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7027 (ΔF1 = -0.0013), buscando superar F1 = 0.7035\n",
      "   - Eliminando mlp_lda → F1 = 0.7032 (ΔF1 = -0.0008), buscando superar F1 = 0.7035\n",
      "   - Eliminando knn_vader → F1 = 0.7041 (ΔF1 = +0.0002), buscando superar F1 = 0.7035\n",
      "   - Eliminando random_forest_vader → F1 = 0.7055 (ΔF1 = +0.0015), buscando superar F1 = 0.7041\n",
      "   - Eliminando logreg_lda → F1 = 0.7078 (ΔF1 = +0.0038), buscando superar F1 = 0.7055\n",
      "   - Eliminando xgb_vader → F1 = 0.7055 (ΔF1 = +0.0015), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_vader → F1 = 0.7054 (ΔF1 = +0.0014), buscando superar F1 = 0.7078\n",
      "Nuevo mejor score encontrado: 0.7078 con 12 modelos.\n",
      "\n",
      "Paso 2: analizando impacto de cada eliminación teniendo 12 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6866 (ΔF1 = -0.0212), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.6987 (ΔF1 = -0.0090), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7023 (ΔF1 = -0.0054), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7027 (ΔF1 = -0.0051), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7037 (ΔF1 = -0.0041), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.7045 (ΔF1 = -0.0032), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7040 (ΔF1 = -0.0037), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_lda → F1 = 0.7021 (ΔF1 = -0.0056), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_vader → F1 = 0.7004 (ΔF1 = -0.0074), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_vader → F1 = 0.7002 (ΔF1 = -0.0075), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_vader → F1 = 0.7014 (ΔF1 = -0.0063), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_vader → F1 = 0.7053 (ΔF1 = -0.0025), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 3: analizando impacto de cada eliminación teniendo 11 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6916 (ΔF1 = -0.0162), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.6993 (ΔF1 = -0.0085), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7046 (ΔF1 = -0.0032), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7005 (ΔF1 = -0.0073), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7068 (ΔF1 = -0.0010), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.7022 (ΔF1 = -0.0056), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7032 (ΔF1 = -0.0046), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_lda → F1 = 0.7034 (ΔF1 = -0.0043), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_vader → F1 = 0.7023 (ΔF1 = -0.0055), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_vader → F1 = 0.7059 (ΔF1 = -0.0018), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_vader → F1 = 0.7041 (ΔF1 = -0.0036), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 4: analizando impacto de cada eliminación teniendo 10 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6874 (ΔF1 = -0.0203), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.7016 (ΔF1 = -0.0061), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7040 (ΔF1 = -0.0038), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7063 (ΔF1 = -0.0014), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7023 (ΔF1 = -0.0054), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.7023 (ΔF1 = -0.0055), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7016 (ΔF1 = -0.0061), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_lda → F1 = 0.7051 (ΔF1 = -0.0027), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_vader → F1 = 0.7043 (ΔF1 = -0.0035), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_vader → F1 = 0.7021 (ΔF1 = -0.0056), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 5: analizando impacto de cada eliminación teniendo 9 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6881 (ΔF1 = -0.0197), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.6988 (ΔF1 = -0.0090), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7034 (ΔF1 = -0.0044), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7013 (ΔF1 = -0.0064), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7042 (ΔF1 = -0.0036), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.7017 (ΔF1 = -0.0061), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7034 (ΔF1 = -0.0044), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_lda → F1 = 0.7037 (ΔF1 = -0.0041), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_vader → F1 = 0.7013 (ΔF1 = -0.0064), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 6: analizando impacto de cada eliminación teniendo 8 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6825 (ΔF1 = -0.0253), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.7007 (ΔF1 = -0.0070), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7015 (ΔF1 = -0.0063), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7023 (ΔF1 = -0.0054), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7048 (ΔF1 = -0.0030), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.6990 (ΔF1 = -0.0088), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7025 (ΔF1 = -0.0053), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_lda → F1 = 0.6994 (ΔF1 = -0.0084), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 7: analizando impacto de cada eliminación teniendo 7 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6882 (ΔF1 = -0.0196), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.6987 (ΔF1 = -0.0090), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.6998 (ΔF1 = -0.0079), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7031 (ΔF1 = -0.0046), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7022 (ΔF1 = -0.0056), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.7016 (ΔF1 = -0.0062), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_tfidf → F1 = 0.7017 (ΔF1 = -0.0061), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 8: analizando impacto de cada eliminación teniendo 6 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6822 (ΔF1 = -0.0255), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.7005 (ΔF1 = -0.0073), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7026 (ΔF1 = -0.0051), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.7022 (ΔF1 = -0.0056), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7041 (ΔF1 = -0.0037), buscando superar F1 = 0.7078\n",
      "   - Eliminando knn_lda → F1 = 0.6989 (ΔF1 = -0.0088), buscando superar F1 = 0.7078\n",
      "\n",
      "Paso 9: analizando impacto de cada eliminación teniendo 5 modelos.\n",
      "   - Eliminando xgb_tfidf → F1 = 0.6753 (ΔF1 = -0.0325), buscando superar F1 = 0.7078\n",
      "   - Eliminando mlp_tfidf → F1 = 0.6990 (ΔF1 = -0.0087), buscando superar F1 = 0.7078\n",
      "   - Eliminando logreg_tfidf → F1 = 0.7003 (ΔF1 = -0.0075), buscando superar F1 = 0.7078\n",
      "   - Eliminando random_forest_lda → F1 = 0.6954 (ΔF1 = -0.0123), buscando superar F1 = 0.7078\n",
      "   - Eliminando xgb_lda → F1 = 0.7013 (ΔF1 = -0.0065), buscando superar F1 = 0.7078\n",
      "\n",
      "Evaluación (12 modelos):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.79      0.73      0.76      1134\n",
      "             Bipolar       0.79      0.70      0.74       814\n",
      "          Depression       0.69      0.73      0.71      4537\n",
      "              Normal       0.88      0.93      0.90      4987\n",
      "Personality disorder       0.71      0.48      0.57       332\n",
      "              Stress       0.61      0.55      0.58       780\n",
      "            Suicidal       0.70      0.66      0.67      3221\n",
      "\n",
      "            accuracy                           0.76     15805\n",
      "           macro avg       0.74      0.68      0.71     15805\n",
      "        weighted avg       0.76      0.76      0.76     15805\n",
      "\n",
      "Macro Precision: 0.7376\n",
      "Macro Recall:    0.6810\n",
      "Macro F1-Score:  0.7050\n",
      "Macro F1-score: 0.7050\n",
      "\n",
      "Archivo 'metricas_FAN.csv' guardado con éxito.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED) \n",
    "\n",
    "# Cargar del dataset\n",
    "archivo = \"Combined_Data.csv\"\n",
    "datos = pd.read_csv(archivo)\n",
    "col_texto = 'statement'\n",
    "col_clasificacion='status'\n",
    "\n",
    "label_topics = datos[col_clasificacion].unique()\n",
    "n_topics = len(label_topics)\n",
    "\n",
    "# Se eliminan filas donde 'texto' o 'clasificacion' estén vacíos o nulos\n",
    "datos = datos.dropna(subset=[col_texto, col_clasificacion])  # Se eliminan filas con valores NaN\n",
    "datos = datos[datos[col_texto].str.strip() != '']   # Se eliminan filas con texto vacío\n",
    "datos = datos[datos[col_clasificacion].str.strip() != '']      # Se eliminan filas sin clasificación válida\n",
    "\n",
    "datos[col_texto] = datos[col_texto].apply(lambda x: util.preprocess_text(x))\n",
    "\n",
    "\n",
    "start_time = time.time()  \n",
    "\n",
    "all_models = []\n",
    "\n",
    "\n",
    "X_ent, X_val, X_prueba, y_ent, y_val, y_prueba  = util.separa_datos (datos, col_texto, col_clasificacion)\n",
    "# Transformar etiquetas\n",
    "y_train_enc, y_val_enc,  y_test_enc, label_encoder = util.transforma_etiquetas (y_ent, y_val, y_prueba)\n",
    "\n",
    "\n",
    "print(f\"\\nVectorizaciones \")\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, vectorizer_tfidf = util.process_tfidf(X_ent, X_val, X_prueba)\n",
    "X_train_lda, X_val_lda, X_test_lda, vectorizer_lda, lda_model = util.process_lda(X_ent, X_val, X_prueba, n_topics)\n",
    "X_train_vader, X_val_vader, X_test_vader, vader_vectorizer = util.process_vader(X_ent, X_val, X_prueba)\n",
    "\n",
    "\n",
    "#Modelos a entrenar\n",
    "\n",
    "print(f\"Modelos: \")\n",
    "# Random Forest\n",
    "all_models = util.train_random_forest_fijo(X_train_lda, X_val_lda , X_train_vader, X_val_vader, X_train_tfidf, X_val_tfidf, y_train_enc,  y_val_enc  ,all_models)\n",
    "# Logistic Regression\n",
    "all_models = util.train_logreg_models_fijo (X_train_lda, X_val_lda , X_train_vader, X_val_vader, X_train_tfidf, X_val_tfidf, y_train_enc,  y_val_enc ,all_models)\n",
    "# Knn\n",
    "all_models = util.train_knn_models_fijo (X_train_lda, X_val_lda , X_train_vader, X_val_vader, X_train_tfidf, X_val_tfidf, y_train_enc,  y_val_enc  ,all_models)\n",
    "# Mlp\n",
    "all_models = util.train_mlp_models_fijo (X_train_lda, X_val_lda , X_train_vader, X_val_vader, X_train_tfidf, X_val_tfidf, y_train_enc,  y_val_enc ,all_models)\n",
    "# XGB\n",
    "all_models = util.train_xgb_models_fijo (X_train_lda, X_val_lda , X_train_vader, X_val_vader, X_train_tfidf, X_val_tfidf, y_train_enc,  y_val_enc  ,all_models)\n",
    "\n",
    "\n",
    "\n",
    "# se juntan modelos\n",
    "X_prueba_dict = {\n",
    "    \"lda\": X_test_lda,\n",
    "    \"vader\": X_test_vader,\n",
    "    \"tfidf\": X_test_tfidf\n",
    "}\n",
    "\n",
    "# Junta todos los modelos entrenados y los ordena\n",
    "all_models_sorted = sorted(all_models, key=lambda x: x[1], reverse=True)\n",
    "for modelo in all_models_sorted:\n",
    "    model_name = modelo[3]\n",
    "    caracteristica = modelo[2]\n",
    "    f1_score = modelo[1]\n",
    "    print(f\"{model_name} - {caracteristica} - f1-score: {f1_score}\")\n",
    "\n",
    "\n",
    "historial, fan, mejores_modelos = util.ablacion_iterativa_completa(\n",
    "    modelos_ordenados=all_models_sorted[:13],\n",
    "    X_prueba_dict=X_prueba_dict,\n",
    "    y_prueba=y_test_enc,\n",
    "    entrenar_y_evaluar_fn=util.entrenar_y_evaluar_simple,\n",
    "    min_modelos=4,\n",
    "    label_encoder=label_encoder)\n",
    "\n",
    "\n",
    "f1_macro = historial[\"Macro_F1_Score\"].iloc[0]\n",
    "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "tiempo_total = end_time - start_time\n",
    "\n",
    "# Uso de memoria\n",
    "memoria_mb = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "\n",
    "# Guardar métricas en lista\n",
    "metricas_fan = []\n",
    "metricas_fan.append({\n",
    "    \"f1_macro\": f1_macro,\n",
    "    \"tiempo_m\": tiempo_total/60,\n",
    "    \"memoria_mb\": memoria_mb\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "df_metricas = pd.DataFrame(metricas_fan)\n",
    "df_metricas.to_csv(\"metricas_FAN.csv\", index=False)\n",
    "print(\"\\nArchivo 'metricas_FAN.csv' guardado con éxito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion de LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb tfidf\n",
      "mlp tfidf\n",
      "logreg tfidf\n",
      "random_forest lda\n",
      "xgb lda\n",
      "knn lda\n",
      "random_forest tfidf\n",
      "mlp lda\n",
      "knn vader\n",
      "random_forest vader\n",
      "xgb vader\n",
      "mlp vader\n"
     ]
    }
   ],
   "source": [
    "for modelo, f1_score, caracteristica, model_name in mejores_modelos:\n",
    "    print(model_name, caracteristica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtencion de textos candidatos a usar con LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones múltiples generadas y guardadas.\n",
      "Candidatos para LIME generados y guardados.\n"
     ]
    }
   ],
   "source": [
    "resultados = []\n",
    "\n",
    "# Creamos un diccionario para las representaciones de prueba\n",
    "X_prueba_dict = {\n",
    "    \"lda\": X_test_lda,\n",
    "    \"vader\": X_test_vader,\n",
    "    \"tfidf\": X_test_tfidf\n",
    "}\n",
    "\n",
    "X_prueba_texto = X_prueba  \n",
    "\n",
    "# iteracióno de los mejores modelos\n",
    "for modelo, f1_score, caracteristica, model_name in mejores_modelos:\n",
    "    if caracteristica not in X_prueba_dict:\n",
    "        continue  \n",
    "\n",
    "    y_pred = modelo.predict(X_prueba_dict[caracteristica]) # prediccion de acuerdo a la caracteristica correcta\n",
    "    \n",
    "    # regresar a las etiquetas originales\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "    for i, texto_original in enumerate(X_prueba_texto):\n",
    "        resultados.append({\n",
    "            \"texto\": texto_original,\n",
    "            \"modelo\": model_name,\n",
    "            \"caracteristica\": caracteristica,\n",
    "            \"prediccion\": y_pred_labels[i],\n",
    "            \"real\": label_encoder.inverse_transform([y_test_enc[i]])[0]  # Aquí traemos la clase real\n",
    "        })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "df_resultados.to_csv(\"resultados_mejores_modelos.csv\", index=False)\n",
    "print(\"Predicciones múltiples generadas y guardadas.\")\n",
    "\n",
    "# Agrupamos los resultados por cada texto y su clase real, \n",
    "# de manera que tengamos la lista de predicciones de los distintos modelos por texto\n",
    "agrupado = df_resultados.groupby(['texto', 'real'])['prediccion'].apply(list).reset_index()\n",
    "\n",
    "# Agregamos una columna con el número de palabras de cada texto (para filtrar textos demasiado cortos o largos)\n",
    "agrupado['num_palabras'] = agrupado['texto'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Nos quedamos solo con los textos \"interesantes\" (ni muy cortos ni muy largos)\n",
    "agrupado = agrupado[(agrupado['num_palabras'] >= 10) & (agrupado['num_palabras'] <= 100)]\n",
    "\n",
    "# Seleccionamos una muestra aleatoria de textos con consenso y 10 con discrepancia en sus clasificaciones \n",
    "agrupado['consenso'] = agrupado['prediccion'].apply(lambda x: len(set(x)) == 1)\n",
    "casos_consenso = agrupado[agrupado['consenso'] == True].sample(10, random_state=SEED)\n",
    "casos_discrepancia = agrupado[agrupado['consenso'] == False].sample(10, random_state=SEED)\n",
    "\n",
    "# Unimos ambos subconjuntos para generar un set de candidatos para explicación con LIME\n",
    "casos_candidatos = pd.concat([casos_consenso, casos_discrepancia])\n",
    "\n",
    "# Guardamos el archivo con los textos seleccionados para análisis posteriores\n",
    "casos_candidatos.to_csv(\"candidatos_LIME.csv\", index=False)\n",
    "print(\"Candidatos para LIME generados y guardados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion de LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Uso de especializado de LIME para generar las perturbaciones a nivel de palabras\n",
    "explainer = LimeTextExplainer(class_names=label_encoder.classes_)\n",
    "\n",
    "vectorizers = {\n",
    "    \"tfidf\": vectorizer_tfidf,  \n",
    "    \"vader\": vader_vectorizer,\n",
    "    \"lda\": (vectorizer_lda, lda_model) \n",
    "}\n",
    "\n",
    "# se toman las oraciones candidatas \n",
    "for idx, fila in casos_candidatos.iterrows():\n",
    "    texto = fila['texto']\n",
    "    clase_real = fila['real']  # Se obtiene la clase real del texto\n",
    "    print(f\"\\n____________________\\nTexto {idx+1}: {texto}\\n\")\n",
    "    \n",
    "    for modelo, f1_score, caracteristica, model_name in mejores_modelos:\n",
    "        \n",
    "        if caracteristica not in vectorizers:\n",
    "            continue\n",
    "\n",
    "        print(f\"Modelo: {model_name} ({caracteristica})\")\n",
    "\n",
    "        # se construye el predictor que LIME usara para evaluar el modelo sobre cada variante del texto\n",
    "        #     X_text: vectorización del texto original en la representación correspondiente (TF-IDF, VADER o LDA)\n",
    "        #     predictor: función adaptadora que usara LIME  para evaluar variantes del texto\n",
    "        X_text, predictor = util.contruye_predictor_LIME (caracteristica, texto, modelo, vectorizers, label_encoder)\n",
    "        \n",
    "        # Ejecutamos LIME: el cambio crítico -> pedimos explicación de todas las clases\n",
    "        exp = explainer.explain_instance(\n",
    "            texto,                           # texto: parrafo que queremos explicar y entender\n",
    "            predictor,                       \n",
    "            num_features=10,       # Indica cuántas palabras destacará LIME como las que más influyeron en la predicción para este ejemplo\n",
    "            top_labels=len(explainer.class_names)  # top_labels: cantidad de clases para las cuales generará explicación, aquí usamos todas\n",
    "        )\n",
    "        \n",
    "        # Obtenemos la clase predicha por el modelo en turno\n",
    "        pred_clase = modelo.predict(X_text)[0]\n",
    "        pred_clase_nombre = label_encoder.inverse_transform([pred_clase])[0]\n",
    "\n",
    "        # Calculamos el índice posicional para LIME\n",
    "        lime_class_index = list(explainer.class_names).index(pred_clase_nombre)\n",
    "\n",
    "        # Imprimimos todo para análisis\n",
    "        print(f\"Clase real: {clase_real}\")\n",
    "        print(f\"Clase predicha: {pred_clase_nombre}\")\n",
    "\n",
    "        # Mostramos la explicación centrada en la clase predicha\n",
    "        exp.show_in_notebook(labels=[lime_class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor: arm\n",
      "Machine: arm64\n",
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "Architecture: ('64bit', '')\n",
      "Physical cores: 8\n",
      "Total cores (logical): 8\n",
      "Max Frequency: 3204 MHz\n",
      "Min Frequency: 600 MHz\n",
      "Current Frequency: 3204 MHz\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "# Información básica del sistema\n",
    "print(\"Processor:\", platform.processor())\n",
    "print(\"Machine:\", platform.machine())\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Architecture:\", platform.architecture())\n",
    "\n",
    "# Núcleos de CPU\n",
    "print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "print(\"Total cores (logical):\", psutil.cpu_count(logical=True))\n",
    "\n",
    "# Frecuencia\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(\"Max Frequency:\", cpu_freq.max, \"MHz\")\n",
    "print(\"Min Frequency:\", cpu_freq.min, \"MHz\")\n",
    "print(\"Current Frequency:\", cpu_freq.current, \"MHz\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tesisenv)",
   "language": "python",
   "name": "tesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
